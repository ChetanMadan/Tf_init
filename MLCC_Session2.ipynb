{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Supervised,UNsupervised and Reinforcement \n",
    "2 kinds of problems- \n",
    "Regression and Classification  => Sorting \n",
    "Regression is prediction \n",
    "Features - characterstics of the data that uniquely identifies a particular \n",
    "Regression problems may also have features - predicting the yield of a crop taking rain and seed quality as features \n",
    "\n",
    "supervised - \n",
    "    linear regression : regressiion \n",
    "    logistic regression : classification \n",
    "    decision trees  and so on \n",
    "    \n",
    "    \n",
    "    \n",
    "Linear Regression -\n",
    "features(x) and labels(f(x))\n",
    "x - features - this can also have dimensions \n",
    "\n",
    "y=wx+b where w= weight and b=bias\n",
    "\n",
    "\n",
    "loss - the difference between the best fit line achieved and the original line that we had plotted stter\n",
    "\n",
    "    numpy \n",
    "    random \n",
    "    use of sklearn.linear model \n",
    "    LinearRegression \n",
    "\n",
    "    make object of linear regression model \n",
    "    reshaping x and y to make it a 2D array \n",
    "    \n",
    "    reshaping from -1 to 1 \n",
    "    .coef \n",
    "    .intercept \n",
    "    .predict \n",
    "    \n",
    "    \n",
    "    will need atleast more than 1000 data points for DECENT approach \n",
    "\n",
    "Stochastic gradient descent : \n",
    "    \n",
    "    batch - no of data points froma data set you are using to calculate the gradient descent in an interation \n",
    "    to reduce the computational complexity of a large data set \n",
    "    \n",
    "    Stochastic gradient descent - will take 1 data point batch size - calc gradient descent according to it and then \n",
    "    take an average for multiple iterations of a single batch\n",
    "    \n",
    "    How this reduces complexity - batch size > 1 - for say 1000 datapoints i have to still compute the gradient descent \n",
    "    \n",
    "linear regression - \n",
    "1. slope calculation \n",
    "2. gradient descent ka f(x)\n",
    "\n",
    "Effect of learning rate and no of epochs on the model - Hyper parameters  (Note that some do not consider no of epochs a hyper parameter as it is not a part of the hypothesis function)\n",
    "hyper parameters affect the model during run time \n",
    "1. learning rate very less - too long \n",
    "2. learning rate too high - will bounce between the walls\n",
    "\n",
    "No of epochs -\n",
    "very less- is not able to reach the minimum value as it stops somewhere in the middle \n",
    "very high - \n",
    "1st - it will keep bouncing at the minimum point as at that point slope is 0 - so faltu ka time complexity \n",
    "\n",
    "tensor flow - specialised for matrix operations \n",
    "\n",
    "ln[2]:creating the graph \n",
    "ln[5]:executing the graph \n",
    "    \n",
    "\n",
    "tf.estimators() - tf high level API (predict () and fit() are included as a part of it- we did it separately ) \n",
    "#LOGISTIC REGRESSION \n",
    "Classsification \n",
    "\n",
    "1. decision boundary \n",
    "2. why can we  not use linear regression for the same thing - will  try to have maxm points on the same line : we want the opposite \n",
    "    \n",
    "    3. sigmoid function \n",
    "        approaches either 0 or 1 depending upon value of z = w1x1 + w2x2 + ......... wnxn\n",
    "        \n",
    "        now to reduce cost - can we apply MSE - No : nature of the MSE will want the line to pass through max number of points \n",
    "        on the other hand what we need is - the points to be as far away as possible \n",
    "        \n",
    "    so in logistic regression - the endpoint is the computation of the weights using the sigmoid function \n",
    "    \n",
    "    \n",
    "CodeAcademy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
